"""
Test et √©valuation du syst√®me RAG avec RAGAS.

Ce script permet d'√©valuer la qualit√© du syst√®me RAG en utilisant :
- G√©n√©ration automatique d'un dataset de test
- M√©triques RAGAS (Faithfulness, Answer Relevancy, Context Precision/Recall)
- LLM externe (Groq/OpenAI) pour une √©valuation de qualit√©
"""

import os
from dotenv import load_dotenv
from documents import EmbedderRag

from llama_index.core import SimpleDirectoryReader
from langchain_groq import ChatGroq
from langchain_community.embeddings import OllamaEmbeddings

from ragas.testset import TestsetGenerator
from ragas.integrations.llama_index import evaluate
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.metrics import (
    Faithfulness,
    AnswerRelevancy,
    ContextPrecision,
    ContextRecall,
)

# Charger les variables d'environnement
load_dotenv()
API_KEY = os.getenv("GROQ_API_KEY")

if not API_KEY:
    raise ValueError("La cl√© GROQ_API_KEY n'est pas d√©finie dans le fichier .env")


class RAGEvaluator:
    """
    Classe pour √©valuer un syst√®me RAG avec RAGAS.
    """

    def __init__(
        self,
        documents_path: str = "./documents",
        model_name: str = "openai/gpt-oss-20b",
        embed_model_name: str = "bge-m3",
        testset_size: int = 5
    ):
        """
        Initialise l'√©valuateur RAG.

        Args:
            documents_path: Chemin vers les documents
            model_name: Mod√®le Groq pour l'√©valuation
            embed_model_name: Mod√®le d'embedding Ollama
            testset_size: Nombre de questions de test √† g√©n√©rer
        """
        self.documents_path = documents_path
        self.model_name = model_name
        self.embed_model_name = embed_model_name
        self.testset_size = testset_size

        # Initialiser le LLM Groq pour l'√©valuation (LangChain)
        self.llm = ChatGroq(
            model=self.model_name,
            api_key=API_KEY,
            temperature=0.3
        )

        # Initialiser le mod√®le d'embedding local (LangChain)
        self.embed_model = OllamaEmbeddings(
            model=self.embed_model_name,
        )

        print(f"‚úì LLM initialis√©: {self.model_name}")
        print(f"‚úì Embedding initialis√©: {self.embed_model_name}")

    def generate_testset(self):
        """
        G√©n√®re un dataset de test √† partir des documents.

        Returns:
            Dataset de test RAGAS
        """
        print(f"\nüìö Chargement des documents depuis '{self.documents_path}'...")
        documents = SimpleDirectoryReader(self.documents_path).load_data()
        print(f"‚úì {len(documents)} document(s) charg√©(s)")

        print(f"\nüîß Initialisation du g√©n√©rateur de testset...")
        # Wrapper les mod√®les LangChain pour RAGAS
        generator_llm = LangchainLLMWrapper(self.llm)
        generator_embeddings = LangchainEmbeddingsWrapper(self.embed_model)

        generator = TestsetGenerator(
            llm=generator_llm,
            embedding_model=generator_embeddings,
        )

        print(f"\n‚öôÔ∏è  G√©n√©ration de {self.testset_size} questions de test...")
        print("   (Cette op√©ration peut prendre quelques minutes)")
        testset = generator.generate_with_llamaindex_docs(
            documents,
            testset_size=self.testset_size,
        )

        print(f"‚úì Testset g√©n√©r√© avec succ√®s!")
        return testset

    def evaluate_rag(self, testset):
        """
        √âvalue le syst√®me RAG avec les m√©triques RAGAS.

        Args:
            testset: Dataset de test g√©n√©r√©

        Returns:
            R√©sultats de l'√©valuation
        """
        print(f"\nüèóÔ∏è  Construction de l'index vectoriel...")
        embedder = EmbedderRag(
            model_name=self.embed_model_name,
            input_path=self.documents_path
        )
        index = embedder.build_or_load_index()

        print(f"\nüîç Cr√©ation du QueryEngine...")
        query_engine = index.as_query_engine()

        print(f"\nüìä Configuration des m√©triques d'√©valuation...")
        evaluator_llm = LangchainLLMWrapper(self.llm)

        metrics = [
            Faithfulness(llm=evaluator_llm),
            AnswerRelevancy(llm=evaluator_llm),
            ContextPrecision(llm=evaluator_llm),
            ContextRecall(llm=evaluator_llm),
        ]

        print(f"\nüöÄ √âvaluation en cours...")
        print("   M√©triques:")
        print("   - Faithfulness: Fid√©lit√© de la r√©ponse aux documents")
        print("   - Answer Relevancy: Pertinence de la r√©ponse")
        print("   - Context Precision: Pr√©cision du contexte r√©cup√©r√©")
        print("   - Context Recall: Rappel du contexte pertinent")

        ragas_dataset = testset.to_evaluation_dataset()

        result = evaluate(
            query_engine=query_engine,
            metrics=metrics,
            dataset=ragas_dataset,
        )

        return result

    def display_results(self, result):
        """
        Affiche les r√©sultats de l'√©valuation.

        Args:
            result: R√©sultats de l'√©valuation RAGAS
        """
        print("\n" + "="*60)
        print("üìà R√âSULTATS DE L'√âVALUATION")
        print("="*60)

        # Afficher les scores globaux
        if hasattr(result, 'scores'):
            print("\nüéØ Scores moyens:")
            for metric, score in result.scores.items():
                print(f"   {metric}: {score:.4f}")

        # Convertir en DataFrame pour une vue d√©taill√©e
        df = result.to_pandas()
        print("\nüìã R√©sultats d√©taill√©s:")
        print(df.to_string())

        # Sauvegarder les r√©sultats
        output_file = "ragas_evaluation_results.csv"
        df.to_csv(output_file, index=False)
        print(f"\nüíæ R√©sultats sauvegard√©s dans '{output_file}'")

        return df


def main():
    """
    Fonction principale pour ex√©cuter l'√©valuation RAGAS.
    """
    print("="*60)
    print("üß™ √âVALUATION RAG AVEC RAGAS")
    print("="*60)

    # Initialiser l'√©valuateur
    evaluator = RAGEvaluator(
        documents_path="./documents",
        model_name="openai/gpt-oss-20b",  # Mod√®le Groq
        embed_model_name="bge-m3",  # Mod√®le d'embedding local
        testset_size=5  # Nombre de questions √† g√©n√©rer
    )

    # G√©n√©rer le testset
    testset = evaluator.generate_testset()

    # √âvaluer le syst√®me RAG
    result = evaluator.evaluate_rag(testset)

    # Afficher les r√©sultats
    df = evaluator.display_results(result)

    print("\n‚úÖ √âvaluation termin√©e avec succ√®s!")


if __name__ == "__main__":
    main()
