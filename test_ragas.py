"""
Test et √©valuation du syst√®me RAG avec RAGAS.

Ce script permet d'√©valuer la qualit√© du syst√®me RAG en utilisant :
- G√©n√©ration automatique d'un dataset de test
- M√©triques RAGAS (Faithfulness, Answer Relevancy, Context Precision/Recall)
- LLM externe (Groq/OpenAI) pour une √©valuation de qualit√©
"""

import os
import sys

# Fix Windows console encoding for emojis
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')
from dotenv import load_dotenv
from documents import EmbedderRag
from text_cleaner import TextCleaner
from embedding_manager import EmbeddingManager
from llm import LlmManager

from llama_index.core import SimpleDirectoryReader, Settings
from llama_index.core.schema import Document
from llama_index.core.node_parser import SentenceSplitter
from ragas.testset import TestsetGenerator
from ragas.integrations.llama_index import evaluate
from ragas.llms import LlamaIndexLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_openai import OpenAIEmbeddings as LangchainOpenAIEmbeddings
from ragas.metrics import (
    Faithfulness,
    AnswerRelevancy,
    ContextPrecision,
    ContextRecall,
)

# Charger les variables d'environnement
load_dotenv()


class RAGEvaluator:
    """
    Classe pour √©valuer un syst√®me RAG avec RAGAS.
    """

    def __init__(
        self,
        documents_path: str = "./documents",
        testset_size: int = 5,
        convert_to_nodes: bool = True,
        chunk_size: int = 800,
        chunk_overlap: int = 100
    ):
        """
        Initialise l'√©valuateur RAG.

        Args:
            documents_path: Chemin vers les documents
            testset_size: Nombre de questions de test √† g√©n√©rer
            convert_to_nodes: Si True, convertit les documents en chunks avant RAGAS
            chunk_size: Taille des chunks pour SentenceSplitter (match EmbedderRag)
            chunk_overlap: Chevauchement entre chunks (match EmbedderRag)
        """
        self.documents_path = documents_path
        self.testset_size = testset_size
        self.convert_to_nodes = convert_to_nodes
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        # Initialiser le LLM et l'Embedding Model via les managers
        # Cela configure les Settings globaux de LlamaIndex
        self.llm_manager = LlmManager()
        self.embedding_manager = EmbeddingManager()

        self.llm = Settings.llm
        self.embed_model = Settings.embed_model

    def generate_testset(self):
        """
        G√©n√®re un dataset de test √† partir des documents.

        Returns:
            Dataset de test RAGAS
        """
        print(f"\nüìö Chargement des documents depuis '{self.documents_path}'...")
        documents = SimpleDirectoryReader(self.documents_path).load_data()
        print(f"‚úì {len(documents)} document(s) charg√©(s)")

        # Nettoyer les documents pour am√©liorer la qualit√© du testset
        print("\nüßπ Nettoyage des documents...")
        documents = TextCleaner.clean_documents(
            documents,
            remove_urls=True,
            normalize_medical=False
        )
        print("‚úì Documents nettoy√©s")

        # Convertir les documents en chunks plus petits si activ√© (pour √©viter l'erreur headlines)
        if self.convert_to_nodes:
            print("\nüîß Conversion des documents en chunks...")
            splitter = SentenceSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
            )
            nodes = splitter.get_nodes_from_documents(documents)

            # Convertir les nodes en Documents avec m√©tadonn√©es headlines
            chunked_documents = []
            for node in nodes:
                # Cr√©er un nouveau Document √† partir du node
                chunk_doc = Document(
                    text=node.get_content(),
                    metadata={
                        **node.metadata,
                        'headlines': []  # Garantir la pr√©sence de headlines
                    }
                )
                chunked_documents.append(chunk_doc)

            print(f"‚úì {len(chunked_documents)} chunks cr√©√©s avec m√©tadonn√©es headlines")
            input_data = chunked_documents
        else:
            print("‚ö†Ô∏è  Mode documents (pas de pre-chunking)")
            input_data = documents

        print(f"\nüîß Initialisation du g√©n√©rateur de testset...")
        # # Wrapper les mod√®les LangChain pour RAGAS
        # generator_llm = LangchainLLMWrapper(self.llm)
        # generator_embeddings = LangchainEmbeddingsWrapper(self.embed_model)

        generator = TestsetGenerator.from_llama_index(
            llm=self.llm,
            embedding_model=self.embed_model,
        )

        print(f"\n‚öôÔ∏è  G√©n√©ration de {self.testset_size} questions de test...")
        print(f"   Mode: {'Nodes' if self.convert_to_nodes else 'Documents'}")
        print("   (Cette op√©ration peut prendre quelques minutes)")

        testset = generator.generate_with_llamaindex_docs(
            input_data,
            testset_size=self.testset_size,
            transforms=[],  # D√©sactive HeadlinesExtractor et HeadlineSplitter
        )

        print(f"‚úì Testset g√©n√©r√© avec succ√®s!")
        return testset

    def evaluate_rag(self, testset):
        """
        √âvalue le syst√®me RAG avec les m√©triques RAGAS.

        Args:
            testset: Dataset de test g√©n√©r√©

        Returns:
            R√©sultats de l'√©valuation
        """
        print(f"\nüèóÔ∏è  Construction de l'index vectoriel...")
        embedder = EmbedderRag(
            input_path=self.documents_path
        )
        index = embedder.build_or_load_index()

        print(f"\nüîç Cr√©ation du QueryEngine...")
        query_engine = index.as_query_engine(llm=self.llm)

        print(f"\nüìä Configuration des m√©triques d'√©valuation...")
        evaluator_llm = LlamaIndexLLMWrapper(self.llm)

        # Configuration du mod√®le d'embedding pour RAGAS
        # RAGAS utilise Langchain pour les embeddings, nous devons donc wrapper notre mod√®le
        ragas_embeddings = LangchainEmbeddingsWrapper(
            LangchainOllamaEmbeddings(model=self.embed_model_name)
        )

        metrics = [
            Faithfulness(llm=evaluator_llm),
            AnswerRelevancy(llm=evaluator_llm, embeddings=ragas_embeddings),
            ContextPrecision(llm=evaluator_llm),
            ContextRecall(llm=evaluator_llm),
        ]

        print(f"\nüöÄ √âvaluation en cours...")
        print("   M√©triques:")
        print("   - Faithfulness: Fid√©lit√© de la r√©ponse aux documents")
        print("   - Answer Relevancy: Pertinence de la r√©ponse")
        print("   - Context Precision: Pr√©cision du contexte r√©cup√©r√©")
        print("   - Context Recall: Rappel du contexte pertinent")

        ragas_dataset = testset.to_evaluation_dataset()

        result = evaluate(
            query_engine=query_engine,
            metrics=metrics,
            dataset=ragas_dataset,
        )

        return result

    def display_results(self, result):
        """
        Affiche les r√©sultats de l'√©valuation.

        Args:
            result: R√©sultats de l'√©valuation RAGAS
        """
        print("\n" + "="*60)
        print("üìà R√âSULTATS DE L'√âVALUATION")
        print("="*60)

        df = result.to_pandas()

        # Afficher les scores globaux (moyenne de chaque m√©trique)
        if not df.empty:
            print("\nüéØ Scores moyens:")
            # Calculate the mean for numeric columns only
            for metric in df.select_dtypes(include='number').columns:
                mean_score = df[metric].mean(skipna=True)
                print(f"   {metric}: {mean_score:.4f}")

        print("\nüìã R√©sultats d√©taill√©s:")
        print(df.to_string())

        # Sauvegarder les r√©sultats
        output_file = "ragas_evaluation_results.csv"
        df.to_csv(output_file, index=False)
        print(f"\nüíæ R√©sultats sauvegard√©s dans '{output_file}'")

        return df


def main():
    """
    Fonction principale pour ex√©cuter l'√©valuation RAGAS.
    """
    print("="*60)
    print("üß™ √âVALUATION RAG AVEC RAGAS")
    print("="*60)

    # Initialiser l'√©valuateur
    evaluator = RAGEvaluator(
        documents_path="./documents",
        # model_name="openai/gpt-oss-20b",  # Mod√®le Groq
        # embed_model_name="bge-m3",  # Mod√®le d'embedding local
        testset_size=2,  # Nombre de questions √† g√©n√©rer
        convert_to_nodes=True  # Pre-chunking pour √©viter l'erreur headlines
    )

    # G√©n√©rer le testset
    testset = evaluator.generate_testset()

    # √âvaluer le syst√®me RAG
    result = evaluator.evaluate_rag(testset)

    # Afficher les r√©sultats
    df = evaluator.display_results(result)

    print("\n‚úÖ √âvaluation termin√©e avec succ√®s!")


if __name__ == "__main__":
    main()
